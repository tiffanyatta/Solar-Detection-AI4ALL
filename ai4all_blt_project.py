# -*- coding: utf-8 -*-
"""AI4ALL BLT Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mQEVgzBGpMSKIKfn1MbiP6c8xNDJ6XV-
"""

#Week 3 has seaborn command that will show them all together - Correlation matrix shows how each is correlated

import math
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from matplotlib.colors import ListedColormap
from xgboost import XGBClassifier, XGBRegressor
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import make_moons, make_circles, make_classification
from sklearn.neural_network import MLPClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.gaussian_process import GaussianProcessClassifier
from sklearn.gaussian_process.kernels import RBF
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
from sklearn.inspection import DecisionBoundaryDisplay
from sklearn.model_selection import KFold, cross_val_score

#"C:\Users\bsche\OneDrive\Desktop\Dev\AI4ALL\Project\Pasion et al dataset.csv" <-- IDK how to get downloads for us all to see
# df = pd.read_csv("/Pasion et al dataset.csv")
# df = pd.read_csv("/content/sample_data/Pasion et al dataset.csv")

#Nah, we gon do it wit google drive
from google.colab import drive
drive.mount("/content/drive")

data_dir = "/content/drive/MyDrive/Google_Colab/"
!ls $data_dir
print(f"Data directory set to: {data_dir}")

# df = pd.read_csv(f"{data_dir}Pasion et al dataset (AI4ALL).gsheet")
url = "1QZ16T9cNLx-2pKAqvwO_BYOIx5mi3Cvyb4K4LHfo46M" #how is this url working for everyone??
df = pd.read_csv(f"https://docs.google.com/spreadsheets/d/{url}/export?format=csv")

df.columns

df.head()

"""
plt.subplot(2,1,1)

plt.scatter(df.YRMODAHRMI, df.Date)
plt.xlabel('YR-MO-DA-HR-MI')
plt.ylabel('Date')
plt.title('YR-MO-DA-HR-MI vs Date')
plt.xlim(2.017045e11, 2.01713e11)
plt.ylim(2.01705e7, 2.01713e7)
plt.show()

plt.subplot(2,1,2)

plt.scatter(df.YRMODAHRMI, df.Time)
plt.xlabel('YR-MO-DA-HR-MI')
plt.ylabel('Time')
plt.title('YR-MO-DA-HR-MI vs Time')
plt.xlim(2.017045e11, 2.01713e11)
# plt.ylim(2.01705e7, 2.01713e7)
plt.show()
"""

df2 = df.drop(columns=['PolyPwr'])

"""Why isn't the 8 lines horizontal if YRMODAHRMI is more specific?

plt.subplots(math.ceil(df2.columns.size/2),2,figsize=(10,30))
for feature in df2.columns:
  # plt.subplot(df2.columns.get_loc(feature)+1) # why can't I use this if the rows and columns are already defined above??
  plt.subplot(math.ceil(df2.columns.size/2),2,df2.columns.get_loc(feature)+1)
  #Get size to display all
  plt.title(feature)

  plt.scatter(df2[feature], df.PolyPwr)
  #,figsize=(10,10)
"""

plt.hist(df.PolyPwr, bins=100)
plt.xlabel('PolyPwr')
plt.ylabel('Frequency')
plt.title('Frequency of PolyPwr')
plt.show()

dfNumeric = df.drop(columns=["Location", "Season"])
fig, axes = plt.subplots(math.ceil(dfNumeric.columns.size/2),2,figsize=(10,30))
print(axes)
axes = axes.flatten()
print(axes)
for i, ax in enumerate(axes):
  if i < len(dfNumeric.columns): #Not sure why this is needed
    ax.boxplot(dfNumeric.iloc[:,i])
    ax.set_title(f'Frequency of {dfNumeric.columns[i]}')
  else:
    ax.axis('off')
plt.show()
"""
for feature in dfNumeric.columns:
  plt.subplot(math.ceil(dfNumeric.columns.size/2),2,dfNumeric.columns.get_loc(feature)+1)
  axes.boxplot(dfNumeric[feature])
  # dfNumeric.boxplot(feature)
  # plt.boxplot(df[feature])
  # plt.xlabel(feature)
  plt.ylabel('Frequency')
  plt.title(f'Frequency of {feature}')
  plt.show()
  """

"""
We need to change the YRMODAHRMI, Day, Time, and change it so that it will be constant numbers.
Time is now written in HHMM, but we should just do the number of minutes in the day (It will never say 1070 because there are not 70 minutes in an hour)

Time:
Get each digit, and then use the first 2 and multiply them by 60, then add to the first 2 digits to get the number of minutes

Day:
Get the 4 day digits, multiply by 365.25 (round?), next 2 digits multiplied by 30.5?, then add to the last 2 digits to find the total amount of days from 0BC.
Find what the first sample day is. Set that to day 0. This will be done by subtracting each data point (which is now in days), and subtracting the lowest amount of days from each one.

YRMODAHRMI --> Do we even need this if we have the first 2? Also not all the data imported in the spreadsheet
Seasons --> Do we even need this either?
Hour --> Don't need this; I have time

Location --> Do we need this? I feel like looking at it would be good to have even if there are multiple places with similar latitudes and there can be big difference that are based off of location like urban heat island affect, buildings that may produce heat or coldness, etc.
However, we wouldn't be able to put a location like this kind into a model to find the power output
"""

#Time:
timeEx = 1149

hours = int(timeEx/100)
minutes = timeEx % 100

totalMinutes = hours*60 + minutes

print(f"Hours: {hours}, minutes: {minutes}. Total minutes: {totalMinutes}")


#Day:
dateEx = 20171021

years = int(dateEx / 10000)
days = dateEx % 100
months = int((dateEx%10000)/100)

totalDays = (years*365.25) + (months*30.5) + days

print(f"Years: {years}, months: {months}, days: {days}. Total days: {totalDays}")

print(f"Unconverted first date: {df["Date"].min()}")

def convertTime(time):
  hours = int(time/100)
  minutes = time % 100
  totalMinutes = hours*60 + minutes
  totalHours = totalMinutes/60
  return totalHours

def convertDate(date):
  years = int(date / 10000)
  days = date % 100
  months = int((date%10000)/100)
  totalDays = (years*365.25) + (months*30.5) + days
  return totalDays

def dayFromZero(date):
  return convertDate(date) - convertDate(df["Date"].min())

print(convertTime(1430))
print(convertDate(20180407))
print(dayFromZero(20180407))

df3 = df.drop(columns=['YRMODAHRMI', 'Season', 'Location', "Hour"])
df3['Time'] = df3['Time'].apply(convertTime)
df3['Date'] = df3['Date'].apply(dayFromZero)
# df3.head()
# df3 = df3.drop(columns=["PolyPwr"])
df3.head()

plt.subplots(math.ceil(df3.columns.size/2),2,figsize=(10,30))
for feature in df3.columns:
  plt.subplot(math.ceil(df3.columns.size/2),2,df3.columns.get_loc(feature)+1)
  plt.title(feature)
  plt.scatter(df3[feature], df.PolyPwr)

#Boxplot for each month
#Boxplot per hour
#boxplot per location

"""
DecisionBoundaryDisplay.from_estimator(
  estimator: The trained classifier (e.g., a model like LogisticRegression, SVC, etc.).
  X: Input data (array-like of shape (n_samples, n_features)). For visualization, it should ideally have 2 features.
  grid_resolution: (default: 100) The resolution of the grid used to plot the decision boundary.
  response_method: (default: 'auto') Determines whether to use predict_proba, decision_function, or predict to get the response.
  plot_method: (default: 'contourf') The plotting method, either 'contourf' (filled contours) or 'contour' (lines).
  ax: (optional) Matplotlib axes object to plot on.
"""
# DecisionBoundaryDisplay.from_estimator(

# )

"""# USING DF DATAFRAME ####################################################################################################
##########################################################################
##########################################################################

import seaborn as sns
from sklearn.preprocessing import LabelEncoder



names = [
    "Random Forest",
    "Neural Net",
    "AdaBoost",
    "Naive Bayes",
    "QDA",
    "XGBoost",
]

classifiers = [
    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
    MLPClassifier(alpha=1, max_iter=1000),
    AdaBoostClassifier(),
    GaussianNB(),
    QuadraticDiscriminantAnalysis(),
    XGBClassifier(),
]

# adding a column that contains categorical classifications for PolyPwr
df['PolyPwr_class'] = pd.qcut(
    df['PolyPwr'], q=3, labels=['low', 'medium', 'high']
)

# separating features
X = df.drop(columns=['PolyPwr', 'PolyPwr_class'])
y = df['PolyPwr_class']

# ensuring only numerical types are included
X = X.select_dtypes(include='number')

# splitting data set, leaving some for testing and using some for training
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# testing for feature importance
# train a model supporting feature importance
model = RandomForestClassifier(random_state=42)
model.fit(X_train, y_train)

importances = model.feature_importances_

# create a sorted DataFrame for clarity
feat_imp = pd.DataFrame({
    'Feature': X.columns,
    'Importance': importances
}).sort_values(by='Importance', ascending=False)

# display
plt.figure(figsize=(10,6))
sns.barplot(x='Importance', y='Feature', data=feat_imp)
plt.title('Feature Importance for Predicting PolyPwr_class')
plt.xlabel('Importance Score')
plt.ylabel('Feature')
plt.show()

df[['PolyPwr', 'PolyPwr_class']].tail(20)

datasets = [
    ('AmbientTemp', 'Humidity'),
    ('Pressure', 'Cloud.Ceiling'),
    ('Date', 'Time')
]

# transforming polypwr_class into numeric labels (high: 0, low: 1, medium: 2)
encoder = LabelEncoder()
df['PolyPwr_class_num'] = encoder.fit_transform(df['PolyPwr_class'])


figure = plt.figure(figsize=(20, 10))
i = 1

#creating a smaller sample
df_small = df.sample(n=800, random_state=42).copy()

# using numeric column
y_all = df_small['PolyPwr_class_num'].values


for ds_cnt, (f1, f2) in enumerate(datasets):
    # slice your dataframe
    X = df_small[[f1, f2]].values
    y = y_all

    # standardize for mean= 0, std= 1
    X = StandardScaler().fit_transform(X)
    # split data, half is used for training, half for testing
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.4, random_state=42
    )

    # defining boundaries for plot
    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5
    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5

    # setting up colormaps, cm is for decision boundary and cm_bright is for data points
    cm = plt.cm.RdBu
    cm_bright = ListedColormap(["#FF0000", "#00FF00", "#0000FF"])

    # Input data plot
    ax = plt.subplot(len(datasets), len(classifiers) + 1, i)
    if ds_cnt == 0:
        ax.set_title("Input data")
    # plotting train and test points
    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors="k")
    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6, edgecolors="k")
    ax.set_xlim(x_min, x_max)
    ax.set_ylim(y_min, y_max)
    ax.set_xticks(())
    ax.set_yticks(())
    ax.set_ylabel(f"{f2}")
    ax.set_xlabel(f"{f1}")
    i += 1

    # Loop through classifiers
    for name, clf in zip(names, classifiers):
        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)
        clf.fit(X_train, y_train)
        score = clf.score(X_test, y_test)

        # draws decision boundary
        DecisionBoundaryDisplay.from_estimator(
            clf, X, response_method="predict", cmap=cm, alpha=0.8, ax=ax, eps=0.5
        )

        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors="k")
        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6, edgecolors="k")
        ax.set_xlim(x_min, x_max)
        ax.set_ylim(y_min, y_max)
        ax.set_xticks(())
        ax.set_yticks(())
        if ds_cnt == 0:
            ax.set_title(name)
        ax.text(x_max - 0.3, y_min + 0.3, f"{score:.2f}".lstrip("0"), size=12,
                horizontalalignment="right")
        i += 1

plt.tight_layout()
plt.show()

df[['PolyPwr_class', 'PolyPwr_class_num']].tail(10)

X = df.drop(columns=['PolyPwr', 'PolyPwr_class', 'PolyPwr_class_num'])
y = df['PolyPwr_class_num']

X = X.select_dtypes(include='number')
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

for name, clf in zip(names, classifiers):
    clf.fit(X_train, y_train)
    score = clf.score(X_test, y_test)
    print(f"{name}: {score:.3f}")
#Accuracy of classifiers classifying the training data?
"""

# USING DF3 DATAFRAME ##########################
##########################################################################
#########################################################################

import seaborn as sns
from sklearn.preprocessing import LabelEncoder


names = [
    "Random Forest",
    "Neural Net",
    "AdaBoost",
    "Naive Bayes",
    "QDA",
    "XGBoost",
]

classifiers = [
    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
    MLPClassifier(alpha=1, max_iter=1000),
    AdaBoostClassifier(),
    GaussianNB(),
    QuadraticDiscriminantAnalysis(),
    XGBClassifier(),
]

# adding a column that contains categorical classifications for PolyPwr
df3['PolyPwr_class'] = pd.qcut(
    df3['PolyPwr'], q=3, labels=['low', 'medium', 'high']
)

# separating features
X = df3.drop(columns=['PolyPwr', 'PolyPwr_class'])
y = df3['PolyPwr_class']

# ensuring only numerical types are included
X = X.select_dtypes(include='number')

# splitting data set, leaving some for testing and using some for training
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

#######################

# testing for feature importance
# train a model supporting feature importance
model = RandomForestClassifier(random_state=42)
model.fit(X_train, y_train)

importances = model.feature_importances_

# create a sorted DataFrame for clarity
feat_imp = pd.DataFrame({
    'Feature': X.columns,
    'Importance': importances
}).sort_values(by='Importance', ascending=False)

# display
plt.figure(figsize=(10,6))
sns.barplot(x='Importance', y='Feature', data=feat_imp)
plt.title('Random Forest: Feature Importance for Predicting PolyPwr_class')
plt.xlabel('Importance Score')
plt.ylabel('Feature')
plt.show()

#######################

df3['PolyPwr_class_num'] = LabelEncoder().fit_transform(df3['PolyPwr_class']) #turn low, medium, high into numbers for XGBoost, then recreate the variables required
X = df3.drop(columns=['PolyPwr', 'PolyPwr_class', 'PolyPwr_class_num'])
y = df3['PolyPwr_class_num']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)


model2 = XGBClassifier()
model2.fit(X_train, y_train)

importances2 = model2.feature_importances_

feat_imp2 = pd.DataFrame({
    'Feature': X.columns,
    'Importance': importances2
}).sort_values(by='Importance', ascending=False)

plt.figure(figsize=(10,6))
sns.barplot(x='Importance', y='Feature', data=feat_imp2)
plt.title('XGBoost Classifier: Feature Importance for Predicting PolyPwr_class_num')
plt.xlabel('Importance Score')
plt.ylabel('Feature')
plt.show()

########

df3[['PolyPwr', 'PolyPwr_class']].tail(20)

datasets = [
    ('AmbientTemp', 'Humidity'),
    ('Pressure', 'Cloud.Ceiling'),
    ('Date', 'Time'),
    ('Date', 'AmbientTemp'),
    ('Month', 'AmbientTemp')
]

# transforming polypwr_class into numeric labels (high: 0, low: 1, medium: 2)
encoder = LabelEncoder()
df3['PolyPwr_class_num'] = encoder.fit_transform(df3['PolyPwr_class'])


figure = plt.figure(figsize=(20, 10))
i = 1

#creating a smaller sample
df3_small = df3.sample(n=800, random_state=42).copy()

# using numeric column
y_all = df3_small['PolyPwr_class_num'].values


for ds_cnt, (f1, f2) in enumerate(datasets):
    # slice your dataframe
    X = df3_small[[f1, f2]].values
    y = y_all

    # standardize for mean= 0, std= 1
    X = StandardScaler().fit_transform(X)
    # split data, half is used for training, half for testing
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.4, random_state=42
    )

    # defining boundaries for plot
    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5
    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5

    # setting up colormaps, cm is for decision boundary and cm_bright is for data points
    cm = plt.cm.RdBu
    cm_bright = ListedColormap(["#FF0000", "#00FF00", "#0000FF"])

    # Input data plot
    ax = plt.subplot(len(datasets), len(classifiers) + 1, i)
    if ds_cnt == 0:
        ax.set_title("Input data")
    # plotting train and test points
    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors="k")
    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6, edgecolors="k")
    ax.set_xlim(x_min, x_max)
    ax.set_ylim(y_min, y_max)
    ax.set_xticks(())
    ax.set_yticks(())
    ax.set_ylabel(f"{f2}")
    ax.set_xlabel(f"{f1}")
    i += 1

    # Loop through classifiers
    for name, clf in zip(names, classifiers):
        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)
        clf.fit(X_train, y_train)
        score = clf.score(X_test, y_test)

        # draws decision boundary
        DecisionBoundaryDisplay.from_estimator(
            clf, X, response_method="predict", cmap=cm, alpha=0.8, ax=ax, eps=0.5
        )

        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors="k")
        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6, edgecolors="k")
        ax.set_xlim(x_min, x_max)
        ax.set_ylim(y_min, y_max)
        ax.set_xticks(())
        ax.set_yticks(())
        if ds_cnt == 0:
            ax.set_title(name)
        ax.text(x_max - 0.3, y_min + 0.3, f"{score:.2f}".lstrip("0"), size=12,
                horizontalalignment="right")
        i += 1

plt.tight_layout()
plt.show()

df3[['PolyPwr_class', 'PolyPwr_class_num']].tail(10)

X = df3.drop(columns=['PolyPwr', 'PolyPwr_class', 'PolyPwr_class_num'])
y = df3['PolyPwr_class_num']

X = X.select_dtypes(include='number')
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

for name, clf in zip(names, classifiers):
    clf.fit(X_train, y_train)
    score = clf.score(X_test, y_test)
    print(f"{name}: {score:.3f}")
#Accuracy of classifiers classifying the training data?

#difference between using df3 and df: neural net, and QDA are .3 and .2 higher. Rest are unchanged

#Trying to do XGBoost with regression, not classifier: copied from above
X = df3.drop(columns=['PolyPwr', 'PolyPwr_class', 'PolyPwr_class_num'])
y = df3['PolyPwr']

# X = X.select_dtypes(include='number')
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

#XGBClassifier() --> XGBRegressor(n_estimators=100, max_depth=5, learning_rate=0.1, random_state=42)
xgbRegressor = XGBRegressor(n_estimators=150, max_depth=6, learning_rate=0.1, random_state=42)
xgbRegressor.fit(X_train, y_train)
score = xgbRegressor.score(X_test, y_test)
print(f"XGBoost Regression: {score:.3f}")

print(df3.columns,"\n")
print(f"AmbientTemp (air): minimum temp: {df3["AmbientTemp"].min()}\nmaximum temp: {df3["AmbientTemp"].max()}\nI'm guessing this is in \033[1mCelcius\033[0m and its just really hot where they have solar panels\n")
print(f"Pressure: 1013mbar is average atmospheric pressure. IDK what the unit is but it goes from {df3['Pressure'].min()} to {df3['Pressure'].max()}\n")
not722 = df3[df3["Cloud.Ceiling"] <722.0]
print(f"Cloud.Ceiling: max (not including the 722 numbers which idk what they represent): {not722["Cloud.Ceiling"].max()}. 722 represents {((df3["Cloud.Ceiling"].count() - not722["Cloud.Ceiling"].count())/df3["Cloud.Ceiling"].count()):.4f} fraction of the original sample")
df3.sample(n = 5, random_state = 1)

#Example to see if XGBRegressor works

#Example 1: trying to get a high energy output
highExample = np.array([[300, 13, 30, -60, 100, 7, 50, 50, 10, 10, 1013, 722]])  # 1 sample, 12 features. Doesn't include PolyPwr or alternatives
print(f"High predicted energy output: {xgbRegressor.predict(highExample)}")
lowExample = np.array([[100, 10.5, 60, -60, 1200, 1, 50, 0, 10, 5, 1013, 722]]) #idk why i need double brackets
print(f"Low predicted energy output: {xgbRegressor.predict(lowExample)}")

#we need to now see if we put in an example of these are our variables, now what is the energy output?
#what do we code?

"""***okay so bascially we this code compares Linear Regression, Ridge, and Lasso models by preparing the datset and tells us which predicts the PolyPwr more accurelty"""

from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.model_selection import train_test_split
from xgboost import XGBRegressor

# Define a list of regression models to evaluate
regressors = [
    ("Linear Regression", LinearRegression()),
    ("Ridge", Ridge()),
    ("Lasso", Lasso()),
    ("XGBoost Regression", XGBRegressor(n_estimators=150, max_depth=6, learning_rate=0.1, random_state=42))
]

# Separate features (X) and target (y)
X = df3.drop(columns=['PolyPwr', 'PolyPwr_class', 'PolyPwr_class_num'])
y = df3['PolyPwr']

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Iterate through the models, train, and evaluate
for name, regressor in regressors:
    regressor.fit(X_train, y_train)
    score = regressor.score(X_test, y_test)
    print(f"{name}: {score:.3f}")

df3.shape
#pls read when output of this cell is given
# Each model prints its R² score, which measures how well it predicts PolyPwr (1.0 = perfect).
# Linear, Ridge, and Lasso all score around 0.47, meaning they explain ~47% of the variance.
# Lasso gives a convergence warning because the features aren’t scaled, so it struggles to optimize.
# XGBoost performs best (0.68) because it captures non-linear patterns and doesn’t require feature scaling.

#Seaborn Correlation matrix
from string import ascii_letters

# Compute the correlation matrix
corr = df3.drop(columns=["PolyPwr_class"]).corr()

# Generate a mask for the upper triangle
mask = np.triu(np.ones_like(corr, dtype=bool))

# Set up the matplotlib figure
f, ax = plt.subplots(figsize=(11, 9))

# Generate a custom diverging colormap
cmap = sns.diverging_palette(230, 20, as_cmap=True)

# Draw the heatmap with the mask and correct aspect ratio
sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,
            square=True, linewidths=.5, cbar_kws={"shrink": .5})

corr = df3.drop(columns=["PolyPwr_class", "Altitude", "PolyPwr_class_num"]).corr()
mask = np.triu(np.ones_like(corr, dtype=bool))
f, ax = plt.subplots(figsize=(11, 9))
cmap = sns.diverging_palette(230, 20, as_cmap=True)
sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,
            square=True, linewidths=.5, cbar_kws={"shrink": .5})

df = pd.DataFrame({
    'values': list(y_train) + list(y_test),
    'dataset': ['y_train'] * len(y_train) + ['y_test'] * len(y_test)
})

plt.figure(figsize=(8, 5))
sns.boxplot(data=df, x='dataset', y='values')
plt.title('Boxplots of y_train and y_test')
plt.show()

kf = KFold(n_splits=5, shuffle=True, random_state=42)

scores = cross_val_score(xgbRegressor, X, y, cv=kf)

print("Cross-validation scores:", scores)
print("Mean score:", scores.mean())